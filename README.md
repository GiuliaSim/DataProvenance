# DataProvenance
This repository contains the project code related to data provenance in the context of data preprocessing in data science: *Capturing and querying fine-grained provenance of preprocessing pipelines in data science*.
The code is written in [Python](https://www.python.org/).

## Organization of the repository
This repository is organized in two main sections (folders):
1. [prov_acquisition](prov_acquisition/) contains the part relating to the acquisition of data provenance. 
 This folder is divided into three other sections:
    * [prov_libraries](prov_acquisition/prov_libraries/) - the two implementations for acquiring data provenance. 
    * [real_world_pipeline](prov_acquisition/real_world_pipeline/) - acquisition of the provenance of data on three real pipelines involving different types of preprocessing steps.
    * [preprocessing_methods](prov_acquisition/preprocessing_methods/) - examples of provenance acquisition for each preprocessing operation implemented on datasets generated by *DIGen*, the data generator provided by the [TPC](http://www.tpc.org/tpcdi/).
2. [queries](queries/) contains the code for querying the data provenance.

## Libraries needed to run the code

* [PROV Python library](https://pypi.org/project/prov/): an implementation of the World Wide Web Consortium [Provenance Data Model](https://www.w3.org/TR/prov-dm/).
* [pymongo](https://pymongo.readthedocs.io/en/stable/)
* [pandas](https://pandas.pydata.org/)

## Execution

### Acquisition
1.	To run the examples on the three real pipelines:
    * Go to the *prov_acquisition/* folder
    * Run the file with the python command `python real_world_pipeline/*.py` followed by `-op` if you want to use the optimized implementation.
    For example, **python real_world_pipeline/GermanCleanup_prov.py -op**
    
    
The result will be stored in the *prov_acquisition/prov_results/<nome_dataset>* folder.
For example, **prov_acquisition/prov_results/German**.

2. To run the examples of the DIGen generated datasets:
    * Go to the *prov_acquisition/* folder
    * Run the file with the python command `python preprocessing_methods/*.py -i <input_file>` followed by `-op` if you want to use the optimized implementation.
    For example, **python preprocessing_methods/FT_prov.py -i datasets/Trade_SF3.csv -op**
    
Again the result will be stored in the *prov_acquisition/prov_results/<nome_dataset>* folder.
For example, **prov_acquisition/prov_results/Trade_SF3**.

NOTE: TPC-DI datasets are not present in this repository. They must be generated through the DIGen generator.


### Query
To be able to query the data provenance, the data created in the acquisition phase is inserted into the [MongoDB](https://www.mongodb.com/) database:
1. Create the database with the python command `python queries/create_mongodb.py <db_name> <files_path>`.
For example, **python queries/create_mongodb.py German prov_acquisition/prov_results/German**.
2.	In order to execute the queries it is also necessary to generate the output collection that contains the output entities of the preprocessing pipeline. Indexes are also created to optimize queries. 
      * Go to the *queries/* folder
      * Run the python `python get_output_entities.py <db_name>` command
      For example, **python get_output_entities.py German**
3. Finally, you can run queries with the python `python *.py <db_name>` command
      For example, **python all_transformations.py German**
      
